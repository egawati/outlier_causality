library('MASS')
# Source: https://psychometroscar.com/simulating-multivariate-outliers/
sim_outliers <- function(N, mu, Sigma, MD) {
m <- length(MD)
n <- length(mu)
mu1 <- 0*mu
print(mu1)
x <- mvrnorm(N-m, mu1, Sigma)
L <- chol(Sigma)
T <- diag(Sigma)
Lambda <- diag(T)%*%t(L)
Y <- matrix(0,m,n)
for (k in 1:m){
u <- mvrnorm(1, mu1, Sigma)
u <- Lambda%*%u
c <- t(mu1)%*%solve(Sigma)%*%mu1-MD[k]**2
b <- t(mu1)%*%solve(Sigma)%*%u
a <- t(u)%*%solve(Sigma)%*%u
root <- (-b+sqrt(b**2-4*a*c))/(2*a)
Y[k,] <- root[1]*u
}
x <- rbind(x,Y) + sample(mu, N, replace=TRUE)
return(x)
}
### EXAMPLE ###
N <- 500
Sigma <- matrix(c(1,0.5,0.5,0.5,1,0.5,0.5, 0.5, 1), 3,3)
mu <- c(X = 0, Y = 0, Z=0)
MD <- c(10, 10, 10) ## these are the Mahalanobis' distances
x <- sim_outliers(N, mu, Sigma, MD)
# source: https://stats.stackexchange.com/questions/124538/how-to-generate-a-large-full-rank-random-correlation-matrix-with-some-strong-cor
gen_correlation_matrix <- function(d, k){
# generate a random correlation matrix C of dxd
# d number of dimensions
# k number of factors (k large eigenvalues)
W = replicate(k, rnorm(d))
#create a diagonal matrix (a matrix in which the entries outside the main diagonal are all zero)
D = diag(runif(d),nrow=d)
# create a full rank matrix
S = W%*%t(W) + D
# now generate a symmetric positive-definite matrix (all eigenvalues real and positive)
C = diag(1/sqrt(diag(S)))%*%S%*%diag(1/sqrt(diag(S)))
return(C)
}
get_eigen_value_proportion <- function(eig, d){
proportion = rep(NA, d)
acc_proportion = rep(NA, d)
acc = 0
for(j in 1:m){rep(NA, d)
if(cov){
proportion[j] = eig$values[j] / sum_trS
}else{
proportion[j] = eig$values[j] / p
}
acc = acc + proportion[j]
acc_proportion[j] = acc
}
return(list("proportion of total variance" = proportion,
"accumulative proportion of total variance" = acc_proportion,
))
}
d = 10
k = 2
C = gen_correlation_matrix(d,k)
sd = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
library('MBESS')
C = cor2cov(cor.mat=S, sd=sd)
eig = eigen(C)
result = get_eigen_value_proportion(eig, d)
library('MASS')
# Source: https://psychometroscar.com/simulating-multivariate-outliers/
sim_outliers <- function(N, mu, Sigma, MD) {
m <- length(MD)
n <- length(mu)
mu1 <- 0*mu
print(mu1)
x <- mvrnorm(N-m, mu1, Sigma)
L <- chol(Sigma)
T <- diag(Sigma)
Lambda <- diag(T)%*%t(L)
Y <- matrix(0,m,n)
for (k in 1:m){
u <- mvrnorm(1, mu1, Sigma)
u <- Lambda%*%u
c <- t(mu1)%*%solve(Sigma)%*%mu1-MD[k]**2
b <- t(mu1)%*%solve(Sigma)%*%u
a <- t(u)%*%solve(Sigma)%*%u
root <- (-b+sqrt(b**2-4*a*c))/(2*a)
Y[k,] <- root[1]*u
}
x <- rbind(x,Y) + sample(mu, N, replace=TRUE)
return(x)
}
### EXAMPLE ###
N <- 500
Sigma <- matrix(c(1,0.5,0.5,0.5,1,0.5,0.5, 0.5, 1), 3,3)
mu <- c(X = 0, Y = 0, Z=0)
MD <- c(10, 10, 10) ## these are the Mahalanobis' distances
x <- sim_outliers(N, mu, Sigma, MD)
# source: https://stats.stackexchange.com/questions/124538/how-to-generate-a-large-full-rank-random-correlation-matrix-with-some-strong-cor
gen_correlation_matrix <- function(d, k){
# generate a random correlation matrix C of dxd
# d number of dimensions
# k number of factors (k large eigenvalues)
W = replicate(k, rnorm(d))
#create a diagonal matrix (a matrix in which the entries outside the main diagonal are all zero)
D = diag(runif(d),nrow=d)
# create a full rank matrix
S = W%*%t(W) + D
# now generate a symmetric positive-definite matrix (all eigenvalues real and positive)
C = diag(1/sqrt(diag(S)))%*%S%*%diag(1/sqrt(diag(S)))
return(C)
}
get_eigen_value_proportion <- function(C, d){
eig = eigen(C)
p = nrow(C)
proportion = rep(NA, d)
acc_proportion = rep(NA, d)
acc = 0
for(j in 1:d){rep(NA, d)
proportion[j] = eig$values[j] / p
acc = acc + proportion[j]
acc_proportion[j] = acc
}
return(list("proportion of total variance" = proportion,
"accumulative proportion of total variance" = acc_proportion,
))
}
d = 10
k = 2
C = gen_correlation_matrix(d,k)
sd = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
library('MBESS')
C = cor2cov(cor.mat=S, sd=sd)
eig = eigen(C)
result = get_eigen_value_proportion(eig, d)
library('MASS')
# Source: https://psychometroscar.com/simulating-multivariate-outliers/
sim_outliers <- function(N, mu, Sigma, MD) {
m <- length(MD)
n <- length(mu)
mu1 <- 0*mu
print(mu1)
x <- mvrnorm(N-m, mu1, Sigma)
L <- chol(Sigma)
T <- diag(Sigma)
Lambda <- diag(T)%*%t(L)
Y <- matrix(0,m,n)
for (k in 1:m){
u <- mvrnorm(1, mu1, Sigma)
u <- Lambda%*%u
c <- t(mu1)%*%solve(Sigma)%*%mu1-MD[k]**2
b <- t(mu1)%*%solve(Sigma)%*%u
a <- t(u)%*%solve(Sigma)%*%u
root <- (-b+sqrt(b**2-4*a*c))/(2*a)
Y[k,] <- root[1]*u
}
x <- rbind(x,Y) + sample(mu, N, replace=TRUE)
return(x)
}
### EXAMPLE ###
N <- 500
Sigma <- matrix(c(1,0.5,0.5,0.5,1,0.5,0.5, 0.5, 1), 3,3)
mu <- c(X = 0, Y = 0, Z=0)
MD <- c(10, 10, 10) ## these are the Mahalanobis' distances
x <- sim_outliers(N, mu, Sigma, MD)
# source: https://stats.stackexchange.com/questions/124538/how-to-generate-a-large-full-rank-random-correlation-matrix-with-some-strong-cor
gen_correlation_matrix <- function(d, k){
# generate a random correlation matrix C of dxd
# d number of dimensions
# k number of factors (k large eigenvalues)
W = replicate(k, rnorm(d))
#create a diagonal matrix (a matrix in which the entries outside the main diagonal are all zero)
D = diag(runif(d),nrow=d)
# create a full rank matrix
S = W%*%t(W) + D
# now generate a symmetric positive-definite matrix (all eigenvalues real and positive)
C = diag(1/sqrt(diag(S)))%*%S%*%diag(1/sqrt(diag(S)))
return(C)
}
get_eigen_value_proportion <- function(C, d){
eig = eigen(C)
p = nrow(C)
proportion = rep(NA, d)
acc_proportion = rep(NA, d)
acc = 0
for(j in 1:d){rep(NA, d)
proportion[j] = eig$values[j] / p
acc = acc + proportion[j]
acc_proportion[j] = acc
}
return(list("proportion of total variance" = proportion,
"accumulative proportion of total variance" = acc_proportion,
))
}
d = 10
k = 2
C = gen_correlation_matrix(d,k)
sd = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
library('MBESS')
S = cor2cov(cor.mat=C, sd=sd)
eig = eigen(C)
result = get_eigen_value_proportion(eig, d)
C
library('MASS')
# Source: https://psychometroscar.com/simulating-multivariate-outliers/
sim_outliers <- function(N, mu, Sigma, MD) {
m <- length(MD)
n <- length(mu)
mu1 <- 0*mu
print(mu1)
x <- mvrnorm(N-m, mu1, Sigma)
L <- chol(Sigma)
T <- diag(Sigma)
Lambda <- diag(T)%*%t(L)
Y <- matrix(0,m,n)
for (k in 1:m){
u <- mvrnorm(1, mu1, Sigma)
u <- Lambda%*%u
c <- t(mu1)%*%solve(Sigma)%*%mu1-MD[k]**2
b <- t(mu1)%*%solve(Sigma)%*%u
a <- t(u)%*%solve(Sigma)%*%u
root <- (-b+sqrt(b**2-4*a*c))/(2*a)
Y[k,] <- root[1]*u
}
x <- rbind(x,Y) + sample(mu, N, replace=TRUE)
return(x)
}
### EXAMPLE ###
N <- 500
Sigma <- matrix(c(1,0.5,0.5,0.5,1,0.5,0.5, 0.5, 1), 3,3)
mu <- c(X = 0, Y = 0, Z=0)
MD <- c(10, 10, 10) ## these are the Mahalanobis' distances
x <- sim_outliers(N, mu, Sigma, MD)
# source: https://stats.stackexchange.com/questions/124538/how-to-generate-a-large-full-rank-random-correlation-matrix-with-some-strong-cor
gen_correlation_matrix <- function(d, k){
# generate a random correlation matrix C of dxd
# d number of dimensions
# k number of factors (k large eigenvalues)
W = replicate(k, rnorm(d))
#create a diagonal matrix (a matrix in which the entries outside the main diagonal are all zero)
D = diag(runif(d),nrow=d)
# create a full rank matrix
S = W%*%t(W) + D
# now generate a symmetric positive-definite matrix (all eigenvalues real and positive)
C = diag(1/sqrt(diag(S)))%*%S%*%diag(1/sqrt(diag(S)))
return(C)
}
get_eigen_value_proportion <- function(C, d){
eig = eigen(C)
p = nrow(C)
proportion = rep(NA, d)
acc_proportion = rep(NA, d)
acc = 0
for(j in 1:d){rep(NA, d)
proportion[j] = eig$values[j] / p
acc = acc + proportion[j]
acc_proportion[j] = acc
}
return(list("proportion of total variance" = proportion,
"accumulative proportion of total variance" = acc_proportion,
))
}
d = 10
k = 2
C = gen_correlation_matrix(d,k)
sd = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
library('MBESS')
S = cor2cov(cor.mat=C, sd=sd)
eig = eigen(C)
result = get_eigen_value_proportion(C, d)
library('MASS')
# Source: https://psychometroscar.com/simulating-multivariate-outliers/
sim_outliers <- function(N, mu, Sigma, MD) {
m <- length(MD)
n <- length(mu)
mu1 <- 0*mu
print(mu1)
x <- mvrnorm(N-m, mu1, Sigma)
L <- chol(Sigma)
T <- diag(Sigma)
Lambda <- diag(T)%*%t(L)
Y <- matrix(0,m,n)
for (k in 1:m){
u <- mvrnorm(1, mu1, Sigma)
u <- Lambda%*%u
c <- t(mu1)%*%solve(Sigma)%*%mu1-MD[k]**2
b <- t(mu1)%*%solve(Sigma)%*%u
a <- t(u)%*%solve(Sigma)%*%u
root <- (-b+sqrt(b**2-4*a*c))/(2*a)
Y[k,] <- root[1]*u
}
x <- rbind(x,Y) + sample(mu, N, replace=TRUE)
return(x)
}
### EXAMPLE ###
N <- 500
Sigma <- matrix(c(1,0.5,0.5,0.5,1,0.5,0.5, 0.5, 1), 3,3)
mu <- c(X = 0, Y = 0, Z=0)
MD <- c(10, 10, 10) ## these are the Mahalanobis' distances
x <- sim_outliers(N, mu, Sigma, MD)
# source: https://stats.stackexchange.com/questions/124538/how-to-generate-a-large-full-rank-random-correlation-matrix-with-some-strong-cor
gen_correlation_matrix <- function(d, k){
# generate a random correlation matrix C of dxd
# d number of dimensions
# k number of factors (k large eigenvalues)
W = replicate(k, rnorm(d))
#create a diagonal matrix (a matrix in which the entries outside the main diagonal are all zero)
D = diag(runif(d),nrow=d)
# create a full rank matrix
S = W%*%t(W) + D
# now generate a symmetric positive-definite matrix (all eigenvalues real and positive)
C = diag(1/sqrt(diag(S)))%*%S%*%diag(1/sqrt(diag(S)))
return(C)
}
get_eigen_value_proportion <- function(C, d){
eig = eigen(C)
p = nrow(C)
proportion = rep(NA, d)
acc_proportion = rep(NA, d)
acc = 0
for(j in 1:d){rep(NA, d)
proportion[j] = eig$values[j] / p
acc = acc + proportion[j]
acc_proportion[j] = acc
}
return(list("proportion of total variance" = proportion,
"accumulative proportion of total variance" = acc_proportion
))
}
d = 10
k = 2
C = gen_correlation_matrix(d,k)
sd = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
library('MBESS')
S = cor2cov(cor.mat=C, sd=sd)
eig = eigen(C)
result = get_eigen_value_proportion(C, d)
result
d = 10
k = 2
C = gen_correlation_matrix(d,k)
sd = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
library('MBESS')
S = cor2cov(cor.mat=C, sd=sd)
eig = eigen(C)
result = get_eigen_value_proportion(C, d)
result
d = 10
k = 1
C = gen_correlation_matrix(d,k)
sd = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
library('MBESS')
S = cor2cov(cor.mat=C, sd=sd)
eig = eigen(C)
result = get_eigen_value_proportion(C, d)
result
d = 10
k = 5
C = gen_correlation_matrix(d,k)
sd = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
library('MBESS')
S = cor2cov(cor.mat=C, sd=sd)
eig = eigen(C)
result = get_eigen_value_proportion(C, d)
result
cor(x)
cov(x)
eigen(cor(x))
knitr::opts_chunk$set(echo = TRUE)
df_case1 = read.csv('dbpca/pickles/performance/Case1/aggregate_3_1K_Case1_30.csv')
df_case1 = read.csv('dbpca/pickles/performance/small_cases/Case1/aggregate_3_1K_Case1_30.csv')
df_case1
df_case4 = read.csv('dbpca/pickles/performance/small_cases/Case4/aggregate_3_1K_Case4_30.csv')
df_case4
var.test(df_case1$precision, df_case4$precision)
t.test(df_case1$precision, df_case4$precision, alternative = "greater", var.equal=TRUE)
t.test(df_case1$recall, df_case4$recall, alternative = "greater", var.equal=TRUE)
seq(0.5, 0.4)
seq(0.1, 0.15)
seq(0.1, 1, 0.1)
knitr::opts_chunk$set(echo = TRUE)
df_case1 = read.csv('dbpca/pickles/performance/small_cases/Case1/aggregate_3_1K_Case1_30.csv')
df_case1
df_case4 = read.csv('dbpca/pickles/performance/small_cases/Case4/aggregate_3_1K_Case4_30.csv')
df_case4
var.test(df_case1$precision, df_case4$precision)
t.test(df_case1$precision, df_case4$precision, alternative = "greater", var.equal=TRUE)
var.test(df_case1$recall, df_case4$recall)
t.test(df_case1$recall, df_case4$recall, alternative = "greater", var.equal=TRUE)
var.test(df_case1$f1_score, df_case4$f1_score)
t.test(df_case1$f1_score, df_case4$f1_score, alternative = "greater", var.equal=TRUE)
knitr::opts_chunk$set(echo = TRUE)
df_case1 = read.csv('dbpca/pickles/performance/small_cases/Case1/3_w1K_Case1_1.csv')
df_case1
df_case4 = read.csv('dbpca/pickles/performance/small_cases/Case4/3_w1K_Case4_1.csv')
df_case4
var.test(df_case1$precision, df_case4$precision)
t.test(df_case1$precision, df_case4$precision, alternative = "greater", var.equal=TRUE)
var.test(df_case1$recall, df_case4$recall)
t.test(df_case1$recall, df_case4$recall, alternative = "greater", var.equal=TRUE)
var.test(df_case1$f1_score, df_case4$f1_score)
t.test(df_case1$f1_score, df_case4$f1_score, alternative = "greater", var.equal=TRUE)
df_case1 = read.csv('dbpca/pickles/performance/small_cases/Case1/3_w1K_Case1_1.csv')
df_case1
df_case4 = read.csv('dbpca/pickles/performance/small_cases/Case4/3_w1K_Case4_1.csv')
df_case4
var.test(df_case1$precision, df_case4$precision)
t.test(df_case1$precision, df_case4$precision, alternative = "greater", var.equal=TRUE)
knitr::opts_chunk$set(echo = TRUE)
df_case1 = read.csv('dbpca/pickles/performance/small_cases/Case1/2_w1K_Case1_1.csv')
df_case1
df_case4 = read.csv('dbpca/pickles/performance/small_cases/Case4/2_w1K_Case4_1.csv')
df_case4
var.test(df_case1$precision, df_case4$precision)
t.test(df_case1$precision, df_case4$precision, alternative = "greater", var.equal=TRUE)
var.test(df_case1$recall, df_case4$recall)
t.test(df_case1$recall, df_case4$recall, alternative = "greater", var.equal=TRUE)
var.test(df_case1$f1_score, df_case4$f1_score)
t.test(df_case1$f1_score, df_case4$f1_score, alternative = "greater", var.equal=TRUE)
var.test(df_case1$recall, df_case4$recall)
var.test(df_case1$recall, df_case4$recall)
var.test(df_case1$precision, df_case4$precision)
df_case1 = read.csv('dbpca/pickles/performance/small_cases/Case1/2_w1K_Case1_1.csv')
df_case1
df_case4 = read.csv('dbpca/pickles/performance/small_cases/Case4/2_w1K_Case4_1.csv')
df_case4
var.test(df_case1$precision, df_case4$precision)
t.test(df_case1$precision, df_case4$precision, alternative = "greater", var.equal=TRUE)
var.test(df_case1$recall, df_case4$recall)
t.test(df_case1$recall, df_case4$recall, alternative = "greater", var.equal=TRUE)
t.test(df_case1$f1_score, df_case4$f1_score, alternative = "greater", var.equal=TRUE)
t.test(df_case1$precision, df_case4$precision, alternative = "greater", var.equal=TRUE)
t.test(df_case1$precision, df_case4$precision, alternative = "greater", var.equal=TRUE)
data = rnorm(1000, mean=10, sd=2)
data1 = data[1:200]
data1
mean(data)
mean(data1)
clear
data = rnorm(1000, mean=10, sd=2)
mean(data
)
sd(data)
data1 = data[1:200]
mean(data1)
sd(data1)
data2 = data[201:400]
mean(data2)
sd(data2)
hist(data, freq = FALSE, main = "Histogram and density")
hist(data1, freq = FALSE, main = "Histogram and density")
hist(data, freq = FALSE, main = "Histogram and density")
hist(data1, freq = FALSE, main = "Histogram and density")
hist(data, freq = FALSE, main = "Histogram and density")
hist(data2, freq = FALSE, main = "Histogram and density")
data_exp = rexp(n, rate = 1)
data_exp = rexp(100, rate = 1)
hist(data_exp, freq = FALSE, main = "Histogram and density")
setwd("~/Projects/Codes/outlier_causal/experiments/analysis")
f1_scores_ocular <- c(0.7857142857142857, 1.0, 0.8214285714285714, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7142857142857143)
f1_scores_gcm <- c(0.9285714285714286, 1.0, 0.75, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7857142857142857)
# Calculate the differences between the paired F1 scores
f1_differences <- f1_scores_ocular - f1_scores_gcm
# Perform the paired t-test
result <- t.test(f1_differences)
# Print the t-test result
print(result)
f1_scores_gcm <- c(0.7857142857142857, 1.0, 0.8214285714285714, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7142857142857143)
f1_scores_ocular <- c(0.9285714285714286, 1.0, 0.75, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7857142857142857)
# Calculate the differences between the paired F1 scores
f1_differences <- f1_scores_ocular - f1_scores_gcm
# Perform the paired t-test
result <- t.test(f1_differences)
# Print the t-test result
print(result)
f1_scores_gcm <- c(0.7857142857142857, 1.0, 0.8214285714285714, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7142857142857143)
f1_scores_ocular <- c(0.9285714285714286, 1.0, 0.75, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7857142857142857)
# Calculate the differences between the paired F1 scores
f1_differences <- f1_scores_ocular - f1_scores_gcm
# Perform the paired t-test
result <- t.test(f1_scores_ocular, f1_scores_gcm)
# Print the t-test result
print(result)
f1_scores_gcm <- c(0.7857142857142857, 1.0, 0.8214285714285714, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7142857142857143)
f1_scores_ocular <- c(0.9285714285714286, 1.0, 0.75, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7857142857142857)
# Calculate the differences between the paired F1 scores
f1_differences <- f1_scores_ocular - f1_scores_gcm
# Perform the paired t-test
result_f1 <- t.test(f1_scores_ocular, f1_scores_gcm)
# Print the t-test result
print(result_f1)
avg_exp_time_gcm <- c(2.820896205400001, 8.721629328999999, 3.81172413877778, 3.180053579499999, 2.6012420819999997, 2.617292695571428, 2.7319630608571432, 2.3640152298749997)
avg_exp_time_ocular <- c(1.313183715599998, 4.4166113110000005, 1.41845342488889, 1.289226255199998, 1.2257213712857162, 1.3066303932857106, 1.2450780889999982, 1.1419945210000022)
# Perform the paired t-test
result_avg_exp_time <- t.test(avg_exp_time_ocular, avg_exp_time_scores_gcm)
# Print the t-test result
print(result_avg_exp_time)
f1_scores_gcm <- c(0.7857142857142857, 1.0, 0.8214285714285714, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7142857142857143)
f1_scores_ocular <- c(0.9285714285714286, 1.0, 0.75, 0.9642857142857143, 0.9285714285714286, 0.7142857142857143, 1.0, 0.7857142857142857)
# Calculate the differences between the paired F1 scores
f1_differences <- f1_scores_ocular - f1_scores_gcm
# Perform the paired t-test
result_f1 <- t.test(f1_scores_ocular, f1_scores_gcm)
# Print the t-test result
print(result_f1)
avg_exp_time_gcm <- c(2.820896205400001, 8.721629328999999, 3.81172413877778, 3.180053579499999, 2.6012420819999997, 2.617292695571428, 2.7319630608571432, 2.3640152298749997)
avg_exp_time_ocular <- c(1.313183715599998, 4.4166113110000005, 1.41845342488889, 1.289226255199998, 1.2257213712857162, 1.3066303932857106, 1.2450780889999982, 1.1419945210000022)
# Perform the paired t-test
result_avg_exp_time <- t.test(avg_exp_time_ocular, avg_exp_time_gcm)
# Print the t-test result
print(result_avg_exp_time)
